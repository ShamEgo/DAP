{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cde75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corr_issues = pd.read_excel('meeting_corrosion_rows.xlsx')\n",
    "corr_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e7b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tower_corr = corr_issues['AMSAssetRef'].unique()\n",
    "tower_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Structure Details _ 17March2025.xlsx', sheet_name='Amplitel Structure Details')\n",
    "df = df[df['AMSAssetRef'].isin(tower_corr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['InstallationDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6798a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_date = pd.to_datetime('2001-12-31')\n",
    "\n",
    "df = df[df['InstallationDate'] > cutoff_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(date_val):\n",
    "    if pd.isna(date_val):\n",
    "        return None  # or '' or np.nan, depending on your needs\n",
    "    return date_val.strftime(\"%Y%m%d\")\n",
    "\n",
    "df['Formatted_Installation_Date'] = df['InstallationDate'].apply(convert_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbb598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "# Initialize a list to store individual DataFrames\n",
    "daily_dfs = []\n",
    "\n",
    "# API request setup\n",
    "for i in df.index:\n",
    "    api_url = 'https://www.longpaddock.qld.gov.au/cgi-bin/silo'\n",
    "    \n",
    "    params = {\n",
    "        'format': 'standard',\n",
    "        'lat': df['Latitude'][i],\n",
    "        'lon': df['Longitude'][i],\n",
    "        'start': df['Formatted_Installation_Date'][i],\n",
    "        'finish': '20250620',\n",
    "        'username': 'anubhav.jetley123@gmail.com',\n",
    "        'password': 'apirequest'\n",
    "    }\n",
    "    url = api_url + '/DataDrillDataset.php?' + urllib.parse.urlencode(params)\n",
    "\n",
    "    try:\n",
    "        # Fetch data\n",
    "        with urllib.request.urlopen(url) as remote:\n",
    "            data = remote.read().decode('utf-8')  # Decode bytes to string\n",
    "\n",
    "        # Find the start of the data table (skip metadata)\n",
    "        lines = data.split('\\n')\n",
    "        data_start = 0\n",
    "        for j, line in enumerate(lines):\n",
    "            if line.startswith('Date       Day T.Max'):\n",
    "                data_start = j\n",
    "                break\n",
    "        else:\n",
    "            print(f\"No data table found for index {i}, lat: {df['Latitude'][i]}, lon: {df['Longitude'][i]}\")\n",
    "            continue\n",
    "\n",
    "        # Extract data lines (header + actual data)\n",
    "        data_lines = lines[data_start:]\n",
    "\n",
    "        # Join lines into a single string for Pandas\n",
    "        data_str = '\\n'.join(data_lines)\n",
    "\n",
    "        # Read into DataFrame using space as delimiter\n",
    "        single_df = pd.read_csv(StringIO(data_str), delim_whitespace=True)\n",
    "\n",
    "        # Add a column to identify the source row (optional, for tracking)\n",
    "        single_df['AMSAssetRef'] = df['AMSAssetRef'][i]\n",
    "        single_df['Latitude'] = df['Latitude'][i]\n",
    "        single_df['Longitude'] = df['Longitude'][i]\n",
    "\n",
    "        # Append to the list\n",
    "        daily_dfs.append(single_df)\n",
    "\n",
    "        # Optional: Add a small delay to avoid overwhelming the API\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing index {i}, lat: {df['Latitude'][i]}, lon: {df['Longitude'][i]}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "if daily_dfs:  # Check if the list is not empty\n",
    "    final_df = pd.concat(daily_dfs, ignore_index=True)\n",
    "    final_df.to_csv('silo.csv', index=False)\n",
    "    print(\"Data saved to silo.csv\")\n",
    "else:\n",
    "    print(\"No data was retrieved. Check API response or input data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "\n",
    "# File to save progress\n",
    "checkpoint_file = 'daily_dfs_checkpoint.pkl'\n",
    "last_processed_file = 'last_processed_row.txt'\n",
    "\n",
    "# Load existing progress if available\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        daily_dfs = pickle.load(f)\n",
    "else:\n",
    "    daily_dfs = []\n",
    "\n",
    "# Load last processed row index, default to 0 if not exists\n",
    "if os.path.exists(last_processed_file):\n",
    "    with open(last_processed_file, 'r') as f:\n",
    "        start_index = int(f.read())\n",
    "else:\n",
    "    start_index = 0\n",
    "\n",
    "# Iterate through the DataFrame rows starting from last processed\n",
    "for i in df.index[start_index:]:\n",
    "    try:\n",
    "        url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        params = {\n",
    "            \"latitude\": df['Latitude'][i],\n",
    "            \"longitude\": df['Longitude'][i],\n",
    "            \"start_date\": df['InstallationDate'],\n",
    "            \"end_date\": \"2025-06-20\",\n",
    "            \"hourly\": [\"rain\", \"wind_speed_80m\", \"temperature_80m\"]\n",
    "        }\n",
    "        responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "        # Process first location\n",
    "        response = responses[0]\n",
    "\n",
    "        # Process hourly data\n",
    "        hourly = response.Hourly()\n",
    "        hourly_rain = hourly.Variables(0).ValuesAsNumpy()\n",
    "        hourly_wind_speed_80m = hourly.Variables(1).ValuesAsNumpy()\n",
    "        hourly_temperature_80m = hourly.Variables(2).ValuesAsNumpy()\n",
    "\n",
    "        # Create hourly DataFrame\n",
    "        hourly_data = {\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\"\n",
    "            ),\n",
    "            \"rain\": hourly_rain,\n",
    "            \"wind_speed_80m\": hourly_wind_speed_80m,\n",
    "            \"temperature_80m\": hourly_temperature_80m,\n",
    "            \"AMSAssetRef\": df['AMSAssetRef'][i]\n",
    "        }\n",
    "\n",
    "        hourly_dataframe = pd.DataFrame(data=hourly_data)\n",
    "\n",
    "        # Convert to daily averages\n",
    "        daily_dataframe = hourly_dataframe.resample('D', on='date').agg({\n",
    "            'rain': 'sum',  # Sum for rain as itâ€™s cumulative\n",
    "            'wind_speed_80m': 'mean',\n",
    "            'temperature_80m': 'mean',\n",
    "            'AMSAssetRef': 'first'\n",
    "        }).reset_index()\n",
    "\n",
    "        daily_dfs.append(daily_dataframe)\n",
    "\n",
    "        # Save progress after each successful iteration\n",
    "        with open(checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(daily_dfs, f)\n",
    "        \n",
    "        # Update last processed row\n",
    "        with open(last_processed_file, 'w') as f:\n",
    "            f.write(str(i + 1))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {i}: {e}\")\n",
    "        # Wait for 60 seconds to respect API rate limit\n",
    "        print(\"Waiting 60 seconds due to API limit...\")\n",
    "        time.sleep(60)  # Adjusted to 60 seconds as per your comment\n",
    "        # Resume from the current row\n",
    "        continue\n",
    "\n",
    "# Concatenate all daily DataFrames\n",
    "final_df = pd.concat(daily_dfs, ignore_index=True)\n",
    "print(final_df)\n",
    "\n",
    "# Optionally, save final DataFrame to CSV\n",
    "final_df.to_csv('open_meteo_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
